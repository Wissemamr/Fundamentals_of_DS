{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div align=\"center\">\n",
    "  <h1> Feature selection using information gain </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A glimpse at Entropy  : \n",
    "https://youtu.be/YtebGVx-Fxw?si=8WeYydAUV6rlan4I\n",
    "\n",
    "In 1948, Claude Shannon introduced the conecpt of entropy in his paper \"A Mathematical Theory of Communication,\" . In information theory, entropy is a measure of uncertainty or surprise associated with a random variable.\n",
    "- Entropy formula :\n",
    "$$ H(X) = \\sum_{i=1}^{n} P(x_i) \\cdot \\log \\left(\\frac{1}{P(x_i)}\\right) $$\n",
    "\n",
    "- Conditional entropy formula :\n",
    "$$ H(Y|X) = - \\sum_{i=1}^{m} \\sum_{j=1}^{n} P(x_i, y_j) \\cdot \\log \\left(\\frac{P(x_i, y_j)}{P(x_i)}\\right) $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the context of information gain, entropy is a measure of impurity or uncertainty in a set of data. Specifically, it quantifies the disorder or randomness in a collection of examples within a dataset. In decision tree algorithms, entropy is used to determine the effectiveness of splitting a dataset based on different attributes. The goal is to find the attribute that minimizes entropy, thereby maximizing information gain and improving the efficiency of the decision tree in classifying or predicting outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A glimpse at mutual information :\n",
    "https://youtu.be/eJIp_mgVLwE?si=T6gEhOWskObv_a8s\n",
    "\n",
    "The Mutual Information between two random variables measures non-linear relations between them. Besides, it indicates how much information can be obtained from a random variable by observing another random variable.\n",
    "\n",
    "It is closely linked to the concept of entropy. This is because it can also be known as the reduction of uncertainty of a random variable if another is known. Therefore, a high mutual information value indicates a large reduction of uncertainty whereas a low value indicates a small reduction. If the mutual information is zero, that means that the two random variables are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Exercice 01 : </u><br>\n",
    "\n",
    "Consider the following dataset containing information about the weather and\n",
    "the number of people who visited a park on a given day:\n",
    "| Temperature | Cloud cover |Humidity | Weather |\n",
    "| ----------- | ------------- |-------------|---------- |\n",
    "| 75 | Sunny | Low | Sunny |\n",
    "| 80 | Partly Cloudy | High | Sunny |\n",
    "| 85 | Overcast | High | Rainy |\n",
    "| 70 | Sunny | Medium | Sunny |\n",
    "| 65 | Overcast | Medium | Stormy |\n",
    "| 60 | Partly Cloudy | Low | Sunny |\n",
    "| 90 | Overcast | High | Rainy |\n",
    "\n",
    "Your task is to select the most relevant features (weather conditions) to predict\n",
    "the number of visitors to the park. To do this, you will calculate the information\n",
    "gain for each feature.\n",
    "\n",
    "1. Discretize the feature Temperature; so we have three levels of temperature: warm, hot, and very hot.\n",
    "2. Calculate the Entropy of the target class Weather, using the formula: <br>\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{x=1}^{n} P(x_i) \\cdot \\log_2(P(x_i))\n",
    "$$\n",
    "\n",
    "Where, $P(x_i)$ is the appearance probability of value $x_i$ among the values of the feature $X$, and $n$ is the number of records of $X$.\n",
    "\n",
    "3. Calculate the information gain of each feature in regard to the target\n",
    "   feature Weather, using the information gain formula: <br>\n",
    "\n",
    "   $$\n",
    "   \\text{IG}(A, X) = H(X) - \\sum_{i=1}^{n} P(x_i) \\cdot H(A | x_i)\n",
    "   $$\n",
    "\n",
    "   Where $H(A |x_i)$ is the entropy of the feature $A$ calculated on the portion of data where the target feature has the value $x_i$\n",
    "\n",
    "4. Order the features according to their IG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <U> Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Discretize the feature Temperature; so we have three levels of temperature: warm, hot, and very hot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 80)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_percentiles(lst):\n",
    "    percentile_33 = np.percentile(lst, 33.33)\n",
    "    percentile_66 = np.percentile(lst, 66.66)\n",
    "    return round(percentile_33), round(percentile_66)\n",
    "\n",
    "\n",
    "temp_list = [75, 80, 85, 70, 65, 60, 90]\n",
    "print(calculate_percentiles(temp_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the temperature colummn can be discretized accordingly :  <br>\n",
    "<b>Warm: [60,70[ <br>\n",
    "Hot: [70,80[ <br>\n",
    "Very Hot: [80-90] <br>\n",
    "<br>\n",
    "We obtain this table : <br>\n",
    "\n",
    "| Temperature | Cloud cover   |Humidity | Weather |\n",
    "| ----------- | ------------- |-------------|---------- |\n",
    "| Hot         | Sunny | Low | Sunny |\n",
    "| Very Hot | Partly Cloudy | High | Sunny |\n",
    "| Very Hot | Overcast | High | Rainy |\n",
    "| Hot | Sunny | Medium | Sunny |\n",
    "| Warm | Overcast | Medium | Stormy |\n",
    "| Warm | Partly Cloudy | Low | Sunny |\n",
    "| Very Hot | Overcast | High | Rainy |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Calculate the Entropy of the target class Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "H(X) = -( P(\\text{Sunny}) \\cdot \\log_2(P(\\text{Sunny})) + P(\\text{Rainy}) \\cdot \\log_2(P(\\text{Rainy})) + P(\\text{Stormy}) \\cdot \\log_2(P(\\text{Stormy})) )\n",
    "$$\n",
    "Where $P(\\text{Sunny}) , P(\\text{Rainy}), P(\\text{Stormy})$ ,are the probabilities of each value that the 'Weather' class takes <br>\n",
    "Knowing that : <br>\n",
    "$n= 7$ <br>\n",
    "\n",
    "$P(\\text{Sunny})= \\frac{4}{7} $ <br>\n",
    "$P(\\text{Rainy}) = \\frac{2}{7}$ <br>\n",
    "$P(\\text{Stormy}) = \\frac{1}{7}$ <br>\n",
    "\n",
    "The entoropy of the 'Weather' class is then given by : <br>\n",
    "$$\n",
    "H(\\text{Weather}) = - \\left( \\frac{4}{7} \\cdot \\log_2\\left(\\frac{4}{7}\\right) + \\frac{2}{7} \\cdot \\log_2\\left(\\frac{2}{7}\\right) + \\frac{1}{7} \\cdot \\log_2\\left(\\frac{1}{7}\\right) \\right) \n",
    "$$\n",
    "\n",
    "$$H(\\text{Weather})\\approx1.3788$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-0.27798716415 + -0.35793227671 + -0.31978045024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Calculate the information gain of each feature in regard to the target feature Weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  The <b> 'Cloud Cover' </b> feature : \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "H(\\text{Cloud Cover | Sunny}) &= -\\left(\\frac{2}{3} \\cdot \\log_2\\left(\\frac{2}{3}\\right) + \\frac{1}{3} \\cdot \\log_2\\left(\\frac{1}{3}\\right)\\right) \\\n",
    "&\\approx 0.92\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\text{Cloud Cover | Partly Cloudy}) &= -\\left(\\frac{1}{1} \\cdot \\log_2\\left(\\frac{1}{1}\\right) + 0 \\cdot \\log_2\\left(0\\right)\\right) \\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\text{Cloud Cover | Overcast}) &= -\\left(\\frac{1}{2} \\cdot \\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\cdot \\log_2\\left(\\frac{1}{2}\\right)\\right) \\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IG}(\\text{Cloud Cover, Weather}) &= -1.10 - \\left(\\frac{3}{7} \\cdot 0.92 + \\frac{1}{7} \\cdot 0 + \\frac{2}{7} \\cdot 1\\right) \\\n",
    "&\\approx -1.10 - \\left(0.40 + 0 + 0.29\\right) \\\n",
    "&\\approx -1.10 - 0.69 \\\n",
    "&\\approx -1.79\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The <b> 'Humidity'  </b>feature : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{align*}\n",
    "H(\\text{Humidity | Low}) &= -\\left(\\frac{2}{2} \\cdot \\log_2\\left(\\frac{2}{2}\\right) + 0 \\cdot \\log_2\\left(0\\right) + 0 \\cdot \\log_2\\left(0\\right)\\right) \\\n",
    "&= 0\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\text{Humidity | High}) &= -\\left(\\frac{2}{3} \\cdot \\log_2\\left(\\frac{2}{3}\\right) + \\frac{1}{3} \\cdot \\log_2\\left(\\frac{1}{3}\\right) + 0 \\cdot \\log_2\\left(0\\right)\\right) \\\n",
    "&\\approx 0.92\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\text{Humidity | Medium}) &= -\\left(\\frac{0}{2} \\cdot \\log_2\\left(0\\right) + \\frac{1}{3} \\cdot \\log_2\\left(\\frac{1}{3}\\right) + \\frac{2}{3} \\cdot \\log_2\\left(\\frac{2}{3}\\right)\\right) \\\n",
    "&\\approx 0.92\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IG}(\\text{Humidity, Weather}) &= -1.10 - \\left(0 \\cdot 0 + \\frac{2}{7} \\cdot 0.92 + \\frac{1}{7} \\cdot 0.92\\right) \\\n",
    "&\\approx -1.10 - \\left(0 + 0.26 + 0.13\\right) \\\n",
    "&\\approx -1.49\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The <b>'Temperature' </b> feature : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\text{Temperature | Warm}) &= -\\left(\\frac{2}{3} \\cdot \\log_2\\left(\\frac{2}{3}\\right) + \\frac{1}{3} \\cdot \\log_2\\left(\\frac{1}{3}\\right) + 0 \\cdot \\log_2\\left(0\\right)\\right) \\\n",
    "&\\approx 0.92\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\text{Temperature | Hot}) &= -\\left(\\frac{1}{2} \\cdot \\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{2} \\cdot \\log_2\\left(\\frac{1}{2}\\right) + 0 \\cdot \\log_2\\left(0\\right)\\right) \\\n",
    "&= 1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "H(\\text{Temperature | Very Hot}) &= -\\left(\\frac{1}{2} \\cdot \\log_2\\left(\\frac{1}{2}\\right) + 0 \\cdot \\log_2\\left(0\\right) + \\frac{1}{2} \\cdot \\log_2\\left(\\frac{1}{2}\\right)\\right) \\\n",
    "&\\approx 1\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{IG}(\\text{Temperature, Weather}) &= -1.10 - \\left(\\frac{3}{7} \\cdot 0.92 + \\frac{1}{7} \\cdot 1 + \\frac{2}{7} \\cdot 1\\right) \\\n",
    "&\\approx -1.10 - \\left(0.40 + 0.14 + 0.29\\right) \\\n",
    "&\\approx -1.93\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recapitulate the results : <br>\n",
    "- Information Gain for <b>Cloud Cover </b>: -1.79\n",
    "- Information Gain for <b>Temperature </b>: -1.93\n",
    "- Information Gain for <b>Humidity</b>: -1.49\n",
    "\n",
    "NOTE : The negative sign in Information Gain doesn't affect the ordering; it just indicates a reduction in entropy. The larger (in absolute value) the Information Gain, the more significant the reduction in uncertainty when considering that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion and results interpretation : <br>\n",
    "- The IG values indicate how much uncertainty in predicting the Weather is reduced by considering each feature. Higher IG suggests that a feature is more relevant for predicting the target variable (Weather in this case).\n",
    "\n",
    "- In this dataset, the feature <b>'Temperature' </b> has the highest Information Gain ( |-1.93|), indicating that it is the most relevant feature to choose to best predict the Weather. Second is Cloud Cover with an Information Gain of |-1.79| . Humidity has the lowest Information Gain (|-1.49|) among the three features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
